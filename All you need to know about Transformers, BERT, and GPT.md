All you need to know about Transformers, BERT, and GPT

Here are a bunch of #nlp primers to master the most prevalent model architectures.

ðŸ”¹ Transformers: http://transformer.aman.ai
- Mathematical background (Vectors, Matrix Multiplication, Dot Product, Masking, Sampling)
- Attention (Additive/Multiplicative/Dot Product Attention, Self/Cross-Attention, Multihead Attention)
- Core components of the Transformer Architecture (Embeddings, Positional Encoding, Skip Connections, Layer Normalization, Softmax)
- Top-level Transformer Architecture (Encoder and Decoder stack)
- Implementation details (Byte-Pair Encoding, Teacher Forcing, Label Smoothing)
- Lessons learned (What are Transformers learning? Why is training them so hard?)
- Pros/cons of Transformers relative to CNNs/RNNs
- Relation between Transformers and Graph Neural Networks

ðŸ”¹ BERT: http://bert.aman.ai
- Background: Pre-Training, Transformer Encoder
- Contextualized Embeddings
- Masked Language Modeling (MLM)
- Next Sentence Prediction (NSP)
- BERTâ€™s Encoder Architecture vs. Other Decoder Architectures
- The Strength of Bidirectionality
- Supervised Fine-Tuning

ðŸ”¹ GPT: http://gpt.aman.ai
- Background: Generative Pre-Training, Transformer Decoder
- GPT-1: Improving Language Understanding by Generative Pre-Training
- GPT-2: Language Models are Unsupervised Multitask Learners
- GPT-3: Language Models are Few-Shot Learners